

import numpy as np
import pandas as pd
import stumpy
import torch
import cuml
import matplotlib.pyplot as plt
import time
from kneed import KneeLocator
import warnings
import os
from typing import Dict, Any, Optional, Tuple, List

CONFIG = {
    "window_size_range": range(15, 90, 15),
    "k_range_to_test": range(2, 11),
    "candidate_percentile": 1,
    "kmeans_max_iter_elbow": 25,   # Can increase iterations slightly as it's fast
    "kmeans_max_iter_final": 100,  # Can increase iterations for final clustering
    "random_state": 42,
}

def load_and_prepare_data(filepath: str) -> Optional[np.ndarray]:
    print(f"Loading data from '{filepath}'...")
    try:
        df = pd.read_csv(filepath)
        ts = df['close'].astype(np.float64).values
        print(f"Time series length: {len(ts)}")
        return ts
    except (FileNotFoundError, KeyError) as e:
        print(f"ERROR: Could not load data. {e}")
        return None

def calculate_matrix_profile(ts: np.ndarray, window_size: int) -> np.ndarray:
    print(f"Calculating Matrix Profile for window size {window_size}")
    start_time = time.time()
    if torch.cuda.is_available():
        print("GPU found! Using stumpy.gpu_stump.")
        matrix_profile = stumpy.gpu_stump(ts, m=window_size)
    else:
        print("GPU not found. Using stumpy.stump on CPU.")
        matrix_profile = stumpy.stump(ts, m=window_size)
    print(f"Matrix Profile computed in {time.time() - start_time:.2f} seconds.")
    return matrix_profile

def extract_and_normalize_candidates(matrix_profile: np.ndarray, ts: np.ndarray, window_size: int, percentile_threshold: int = 1) -> Tuple[np.ndarray, np.ndarray]:
    print("Selecting and Normalizing Candidates...")
    mp_values = matrix_profile[:, 0].astype(np.float64)
    if not np.any(np.isfinite(mp_values)):
        return np.array([]), np.array([])
    threshold = np.percentile(mp_values[np.isfinite(mp_values)], percentile_threshold)
    candidate_indices = np.where(mp_values < threshold)[0]
    
    filtered_indices, last_idx = [], -np.inf
    for idx in sorted(candidate_indices):
        if idx >= last_idx + window_size:
            filtered_indices.append(idx)
            last_idx = idx
            
    normalized_candidates, final_original_indices = [], []
    for idx in filtered_indices:
        subsequence = ts[idx : idx + window_size]
        if np.std(subsequence) > 1e-6:
            normalized_candidates.append((subsequence - np.mean(subsequence)) / np.std(subsequence))
            final_original_indices.append(idx)
            
    X = np.array(normalized_candidates)
    print(f"Identified and Z-Normalized {len(X)} candidates.")
    return X, np.array(final_original_indices)

# CUML MODIFICATION: This entire function is rewritten to use GPU
def find_optimal_k_gpu(X: np.ndarray, k_range: range, config: Dict[str, Any], output_plot_path: str) -> Optional[int]:
    print("Finding Optimal K using GPU (cuML KMeans with Euclidean distance)...")
    if len(X) < max(k_range): 
        print("Not enough candidates for clustering.")
        return None

    # CUML requires float32 data. It's much faster to convert once.
    X_gpu = X.astype(np.float32)

    inertias = []
    for k in k_range:
        # Use cuML's KMeans
        kmeans_gpu = cuml.KMeans(
            n_clusters=k,
            max_iter=config["kmeans_max_iter_elbow"],
            random_state=config["random_state"]
        )
        kmeans_gpu.fit(X_gpu)
        inertias.append(kmeans_gpu.inertia_)
    
    try:
        kl = KneeLocator(list(k_range), inertias, curve='convex', direction='decreasing')
        optimal_k = kl.elbow
        if optimal_k:
            print(f"Detected optimal k = {optimal_k}")
            kl.plot_knee()
            plt.title('Elbow Method Analysis (GPU-based)')
            plt.savefig(output_plot_path)
            plt.show()
            plt.close()
        return optimal_k
    except Exception as e:
        print(f"Could not find elbow point: {e}")
        return None

def visualize_and_save_clusters(
    X: np.ndarray, clusters: np.ndarray, centroids: np.ndarray, 
    window_size: int, optimal_k: int, output_path: str
):
    # This function doesn't need changes, as it receives standard NumPy arrays.
    print(f"Visualizing clusters for w={window_size}, k={optimal_k}")
    fig, axs = plt.subplots(optimal_k, 1, figsize=(14, 4 * optimal_k), sharex=True, squeeze=False)
    fig.suptitle(f'Discovered Pattern Clusters (Window Size = {window_size}, k={optimal_k})', fontsize=20, y=1.0)
    
    for i in range(optimal_k):
        ax = axs[i, 0]
        # Ensure we're working with a NumPy array for boolean indexing
        clusters_np = np.asarray(clusters)
        cluster_members = X[clusters_np == i]
        ax.set_title(f"Cluster {i} ({len(cluster_members)} patterns found)")
        for member in cluster_members: ax.plot(member, color='skyblue', alpha=0.3)
        ax.plot(np.ravel(centroids[i]), color='black', linewidth=3, label='Centroid')
        ax.legend()
        ax.grid(True, linestyle='--')
        
    plt.xlabel("Time Steps in Pattern")
    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    plt.savefig(output_path, dpi=150)
    plt.close(fig)
    print(f"Saved cluster visualization to {output_path}")

def main(filepath: str, output_dir: str, config: Dict[str, Any]):
    base_filename = os.path.splitext(os.path.basename(filepath))[0]
    file_specific_output_dir = os.path.join(output_dir, base_filename)
    os.makedirs(file_specific_output_dir, exist_ok=True)
    print(f"Results will be saved in: {file_specific_output_dir}")

    ts = load_and_prepare_data(filepath)
    if ts is None: return

    results_X, results_clusters, results_centroids = [], [], []
    results_indices, results_window_sizes, results_k = [], [], []

    for window_size in config["window_size_range"]:
        print(f"\n{'='*25} PROCESSING WINDOW SIZE: {window_size} {'='*25}")
        
        matrix_profile = calculate_matrix_profile(ts, window_size)
        X, original_indices = extract_and_normalize_candidates(
            matrix_profile, ts, window_size, config["candidate_percentile"]
        )

        if len(X) < max(config["k_range_to_test"]):
            print("Not enough candidates found. Skipping this window size.")
            continue
        
        window_plot_dir = os.path.join(file_specific_output_dir, f"w_{window_size}_plots")
        os.makedirs(window_plot_dir, exist_ok=True)
        elbow_plot_path = os.path.join(window_plot_dir, "elbow_plot.png")

        # CUML MODIFICATION: Call the new GPU-based function
        optimal_k = find_optimal_k_gpu(X, config["k_range_to_test"], config, elbow_plot_path)

        if optimal_k:
            print(f"--- Performing Final Clustering with k={optimal_k} on GPU ---")
            
            # CUML MODIFICATION: Use cuml.KMeans for final clustering
            final_km_gpu = cuml.KMeans(
                n_clusters=optimal_k, 
                max_iter=config["kmeans_max_iter_final"], 
                random_state=config["random_state"]
            )
            
            X_gpu = X.astype(np.float32)
            clusters_gpu = final_km_gpu.fit_predict(X_gpu)
            centroids_gpu = final_km_gpu.cluster_centers_

            # CUML MODIFICATION: Move results from GPU to CPU for plotting and saving
            # .get() transfers the data from the GPU's memory to the CPU's memory (as a NumPy array)
            clusters = clusters_gpu.get()
            centroids = centroids_gpu.get()

            # The rest of the code works with the NumPy arrays `clusters` and `centroids`
            results_X.append(X)
            results_clusters.append(clusters)
            results_centroids.append(centroids)
            results_indices.append(original_indices)
            results_window_sizes.append(window_size)
            results_k.append(optimal_k)

            plot_path = os.path.join(window_plot_dir, "cluster_shapes.png")
            visualize_and_save_clusters(X, clusters, centroids, window_size, optimal_k, plot_path)
    
    if results_window_sizes:
        artifacts_path = os.path.join(file_specific_output_dir, "discovery_artifacts.npz")
        np.savez_compressed(
            artifacts_path,
            X_list=np.array(results_X, dtype=object),
            clusters_list=np.array(results_clusters, dtype=object),
            centroids_list=np.array(results_centroids, dtype=object),
            indices_list=np.array(results_indices, dtype=object),
            window_sizes=np.array(results_window_sizes),
            k_values=np.array(results_k)
        )
        print(f"\nSuccessfully saved all pattern data to: {artifacts_path}")
    else:
        print("\nNo patterns were discovered across any window size. No artifact file created.")

    print(f"\n{'='*30} DISCOVERY COMPLETE {'='*30}")